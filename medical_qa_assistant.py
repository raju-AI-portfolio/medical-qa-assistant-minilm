# -*- coding: utf-8 -*-
"""medical_qa_assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CPbceolDTRD1KD97-02T8Iqq9UkXvU74
"""

# First, add your HuggingFace Token (with Write access) to Google Colab Secrets
# Then, load the token using below code

from huggingface_hub import login
from google.colab import userdata

hf_token = userdata.get('HF_API_KEY')
login(token = hf_token)

#@title Download Dataset
from IPython.display import clear_output
!gdown https://drive.google.com/uc?id=1Q1r1FGDaBQLkvPKMjvgnA2eWBpmMLD57
clear_output()
!ls | grep '.csv'

import pandas as pd

df = pd.read_csv("clean_MedQuAD_autotrain_ready.csv")
df.shape
df.head()

# Load your fine-tuned model

from sentence_transformers import SentenceTransformer

# Load embedding model
fine_tuned_model = SentenceTransformer("rajucanon/mini-project-medical-assistant2")      # <------ change this as per your model id
                                                                                # This finetuned model is standing on
                                                                                # Base Model: sentence-transformers/all-MiniLM-L6-v2

# Load your fine-tuned model

from sentence_transformers import SentenceTransformer

# Load embedding model
untuned_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

def compute_embeddings(x, model):
    return model.encode(x)

# Create Fine-tuned embeddings & Untuned embeddings for the `Question`

df["fine_tuned_embeddings"] = df["Question"].apply(lambda x: compute_embeddings(x, fine_tuned_model))
df["untuned_embeddings"] = df["Question"].apply(lambda x: compute_embeddings(x,untuned_model))

print("Embeddings ready for", len(df), "questions.")

df.head()

import numpy as np

def get_answer(user_question, model, top_k=1):
    # Encode user question
    q_emb = model.encode(user_question)

    if model == fine_tuned_model:
        # Convert embeddings column to numpy array
        all_embs = np.vstack(df["fine_tuned_embeddings"].values)

    else:
        # Convert embeddings column to numpy array
        all_embs = np.vstack(df["untuned_embeddings"].values)


    # Compute cosine similarities (vectorized for speed)
    sims = np.dot(all_embs, q_emb) / (
        np.linalg.norm(all_embs, axis=1) * np.linalg.norm(q_emb)
    )
    #----------------------------------
    # Get indices of top-k matches
    top_idx = np.argsort(sims)[::-1][:top_k]

    # Build response
    results = []
    for idx in top_idx:
        results.append(
            f"**Similarity: {sims[idx]:.2f}** â†’ Answer: {df.iloc[idx]['Answer']}"
        )

# Return matched top 3 answers
    return "\n\n".join(results)

# Test the fine-tuned model

get_answer("Can lifestyle changes reduce the risk of developing type 2 diabetes?", model = fine_tuned_model)

# Test the untuned model

get_answer("Can lifestyle changes reduce the risk of developing type 2 diabetes?", model = untuned_model)

import gradio as gr

# Function to get output from Fine-tuned model
def get_response(input):
    response = get_answer(input, model = fine_tuned_model)
    return response


# Input element
in_ques = gr.Textbox(label="Ask a medical question", placeholder="Type your question here...", lines=5)

# Output element
out_ans = gr.Textbox(label="Answer", lines = 10)


# Create Gradio Interface
iface = gr.Interface(fn = get_response,
                     inputs = [in_ques],
                     outputs = [out_ans],
                     title = "Medical Q&A",
                     description = "Application to answer medical questions using a fine-tuned model",
                     flagging_mode = "never")
# Launch interface
iface.launch()          # set `debug=True` while debugging